<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>1748-7188-4-1 1..19</title>
<meta name="Creator" content="Arbortext Advanced Print Publisher 9.0.220/W Unicode"/>
<meta name="Producer" content="Acrobat Distiller 7.0 (Windows)"/>
<meta name="CreationDate" content=""/>
</head>
<body>
<pre>
Algorithms for Molecular Biology
Research

BioMed Central

Open Access

Auto-validating von Neumann rejection sampling from small
phylogenetic tree spaces
Raazesh Sainudiin*1,2 and Thomas York3,4
Address: 1Department of Statistics, University of Oxford, Oxford, OX1 3TG, UK, 2Biomathematics Research Centre, Department of Mathematics
and Statistics, University of Canterbury, Private Bag 4800, Christchurch, New Zealand, 3Department of Biological Statistics and Computational
Biology, Cornell University, Ithaca, New York 14853, USA and 4Boyce Thompson Institute for Plant Research, Cornell University, Ithaca,
New York 14853, USA
E-mail: Raazesh Sainudiin* - r.sainudiin@math.canterbury.ac.nz; Thomas York - tly2@cornell.edu
*Corresponding author

Published: 07 January 2009
Algorithms for Molecular Biology 2009, 4:1

Received: 5 June 2007
doi: 10.1186/1748-7188-4-1

Accepted: 7 January 2009

This article is available from: http://www.almob.org/content/4/1/1
© 2009 Sainudiin and York; licensee BioMed Central Ltd.
This is an Open Access article distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/2.0),
which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.

Abstract
Background: In phylogenetic inference one is interested in obtaining samples from the posterior
distribution over the tree space on the basis of some observed DNA sequence data. One of the
simplest sampling methods is the rejection sampler due to von Neumann. Here we introduce an
auto-validating version of the rejection sampler, via interval analysis, to rigorously draw samples
from posterior distributions over small phylogenetic tree spaces.
Results: The posterior samples from the auto-validating sampler are used to rigorously (i)
estimate posterior probabilities for different rooted topologies based on mitochondrial DNA from
human, chimpanzee and gorilla, (ii) conduct a non-parametric test of rate variation between
protein-coding and tRNA-coding sites from three primates and (iii) obtain a posterior estimate of
the human-neanderthal divergence time.
Conclusion: This solves the open problem of rigorously drawing independent and identically
distributed samples from the posterior distribution over rooted and unrooted small tree spaces
(3 or 4 taxa) based on any multiply-aligned sequence data.

Background

Obtaining samples from a real-valued target density f• (t)
is a basic problem in statistical estimation. The target
f• (t): T ↦ R maps n-dimensional real points in Rn to
real numbers in R, i.e. t Œ T ⊂ Rn. In Bayesian
phylogenetic estimation, we want to draw independent
and identically distributed samples from a target posterior
density on the space of phylogenetic trees. The standard
point-valued or punctual Monte Carlo methods via
conventional floating-point arithmetic are typically nonrigorous as they do not account for all sources of numerical
errors and are limited to evaluating the target at finitely
many points. The standard approaches to sampling from
the posterior density, especially over phylogenetic trees,

rely on Markov chain Monte Carlo (MCMC) methods.
Despite their asymptotic validity, it is nontrivial to
guarantee that an MCMC algorithm has converged to
stationarity [1], and thus MCMC convergence diagnostics
on phylogenetic tree spaces are heuristic [2].
A more direct sampler that is capable of producing
independent and identically distributed samples from
the target density f• (t):= f(t)/(Nf), by only evaluating the
target shape f(t) without knowing the normalizing
constant N f :=

∫T f (t )dt , is the von Neumann rejection

sampler [3]. However, the limiting step in the rejection
ˆ
sampler is the construction of an envelope function g (t)

Page 1 of 19
(page number not for citation purposes)

Algorithms for Molecular Biology 2009, 4:1

that is not only greater than the target shape f(t):= Nf f•
(t) at every t Œ T , but also easy to normalize and draw
samples from. Moreover, a practical and efficient
envelope function has to be as close to the target shape
as possible from above. When an envelope function is
constructed using point-valued methods, except for
simple classes of targets, one cannot guarantee that the
envelope function dominates the target shape globally.
None of the available samplers can rigorously produce
independent and identically distributed samples from
the posterior distribution over phylogenetic tree spaces,
even for 3 or 4 taxa. We describe a new approach for
rigorously drawing samples from a target posterior
distribution over small phylogenetic tree spaces using
the theory of interval analysis. This method can circumvent the problems associated with (i) heuristic convergence diagnostics in MCMC samplers and (ii) pseudoenvelopes constructed via non-rigorous point-valued
methods in rejection samplers.

http://www.almob.org/content/4/1/1

The rest of the paper is organized as follows. In the
Methods Section, we introduce (i) von Neumann
rejection sampler (RS), (ii) phylogenetic estimation
problem, (iii) interval analysis and (iv) an interval
extension of the rejection sampler called the Moore
rejection sampler (MRS) in honor of Ramon E. Moore.
Moore was one of the influential founders of interval
analysis [6]. In Results Section, we employ MRS to
rigorously draw samples from the posterior density over
small tree spaces. Using one of the earliest primate
mitochondrial DNA data sets we use the posterior
samples to estimate the posterior probability of each
rooted tree topology and conduct a non-parametric test
of rate variation between protein-coding and tRNAcoding sites. Using one of the latest data sets we obtain a
rigorous posterior estimate of the human-neanderthal
divergence time. We can also draw samples from the
space of unrooted triplet and quartet trees. We conclude
after a discussion of the method.

Methods
Informally, our method partitions the domain into
boxes and uses interval analysis to rigorously bound
the target shape in each box; then we use as envelope the
simple function which takes on in each box the upper
bound obtained for that box. It is easy to draw samples
from the density corresponding to this step function
envelope. More formally, the method employs an
interval extension of the target posterior shape f(t):
T ↦ R to produce rigorous enclosures of the range of
f over each interval vector or box in an adaptive
partition T := {t (1) , t (2) ,..., t (|T|)} of the tree space T =
∪i t(i). This partition is adaptively constructed by a
priority queue. The interval extended target shape maps
boxes in T to intervals in R. This image interval provides
an upper bound for the global maximum and a lower
bound for the global minimum of f over each element of
the partition of T . We use this information to construct
an envelope as a simple function over the partition T .
Using the Alias method [4] we efficiently propose
samples from this normalized step-function envelope
for von Neumann rejection sampling.
We call our method auto-validating because we employ
interval methods to rigorously construct the envelope for
a large class of target densities. The method was
described in a more rudimentary form in [5]. Unlike
many conventional samplers, each sample produced by
our method is equivalent to a computer-assisted proof
that it is drawn from the desired target, up to the pseudorandomness of the underlying, deterministic, pseudorandom number generator. MRS 0.1.2, a C++ class
library for statistical set processing is available from
http://www.math.canterbury.ac.nz/~r.sainudiin/codes/
mrs under the terms of the GNU General Public License.

In the following sections, we first introduce the rejection
sampler (RS) due to von Neumann [3]. Secondly, we
describe the basic phylogenetic inference problem (e.g.
[7-9]). Then, we introduce the basic principles of interval
methods (e.g. [6,10-13]). Finally, we construct interval
extensions of RS to rigorously draw independent and
identically distributed samples from small phylogenetic
tree spaces. We leave the formal proofs to the Appendix
for completeness.
Rejection sampler (RS)
Rejection sampling [3] is a Monte Carlo method to draw
independent samples from a target random variable or
random vector T with density f• (t):= f(t)/Nf, where t Œ T
⊂ Rn, i.e. T ~ f•. The challenge is to draw the samples
without any knowledge of the normalizing constant

N f :=

∫T f (t )dt . Typically the target f

•

(t) is any density

that is absolutely continuous with respect to the
Lebesgue measure. The von Neumann rejection sampler
(RS) can produce samples from T ~ f• according to
Algorithm 1 when provided with (i) a fundamental
sampler that can produce independent samples from the
Uniform [0, 1] random variable M with density given by
the indicator function 1[0,1](m): R ↦ R, (ii) a target shape
ˆ
f(t): T ↦ R, (iii) an envelope function g(t ) : T
R,
such that,

ˆ
g(t ) ≥ f (t ) for all t ∈ T,

(1)

ˆ
(iv) a normalizing constant N g := ∫ g(t )dt , (v) a
ˆ
T
proposal density g(t ) := (N g ) −1 g(t ) over T from which
independent samples can be drawn and finally (vi) f(t)
ˆ
and g (t) must be computable for any t Œ T .

Page 2 of 19
(page number not for citation purposes)

Algorithms for Molecular Biology 2009, 4:1

ˆ
input : (i) f; (ii) samplers for V ~ g and M ~ 1[0,1]; (iii) g ;
(iv) integer MaxTrials;
output : (i) possibly one sample t from T ~ f• and (ii) Trials
initialize: Trials ← 0; Success ← false; t ← ∅;
repeat //propose at most MaxTrials times until acceptance
v ← sample(g);

//draw a sample v from RV V with
density g

ˆ
u ← g (v) sample(1[0,1]);
if u ≤ f(v) then

//draw a sample u from RV
U with density 1[0, g( v)]

//accept the proposed v and flag Success

t ← v; Success ← true
end
Trials ← Trials +1;

//track the number of proposal
trials so far

until Trials ≥ MaxTrials or Success = true;
return t and Trials
Algorithm 1: von Neumann RS
We use the Mersenne Twister pseudo-random number
generator [14] to imitate independent samples from M ~
1[0,1]. The random variable T, if generated by Algorithm
ˆ
1, is distributed according to f• (e.g. [15]). Let A( g ) be
the probability that a point proposed according to g gets
accepted as an independent sample from f• through the
ˆ
envelope function g . Observe that the envelope-specific
ˆ
acceptance probability A( g ) is the ratio of the integrals

A( g) =

Nf

∫ f (t )dt
:= T
,
Ng
∫T g(t )dt

http://www.almob.org/content/4/1/1

and the probability distribution over the number of
samples from g to obtain one sample from f• is
ˆ
geometrically distributed with mean 1/A( g ) (e.g. [15]).

Phylogenetic estimation
In this section we briefly review phylogenetic estimation.
A more detailed account can be found in [7-9]. Inferring
the ancestral relationship among a set of extant species
based on their DNA sequences is a basic problem in
phylogenetic estimation. One can obtain the likelihood
of a particular phylogenetic tree that relates the extant
species of interest at its leaves by superimposing a
continuous time Markov chain model of DNA substitution upon that tree. The length of an edge (branch
length) connecting two nodes (species) in the tree
represents the amount of evolutionary time (divergence)
between the two species. The internal nodes represent
ancestral species. During the likelihood computation,
one needs to integrate over all possible states at the
unobserved ancestral nodes.

Next we give a brief introduction to some phylogenetic
nomenclature. A phylogenetic tree is said to be rooted if
one of the internal nodes, say node r, is identified as the
root of the tree, otherwise it is said to be unrooted. The
rooted tree is conventionally depicted with the root node
r at the top. The four topology-labeled, three-leaved,
rooted trees, namely, 0t, 1t, 2t and 3t, with leaf label set
{1, 2, 3}, are depicted in Figure 1(i)–(iv). The unrooted,
three-leaved tree with topology label 4 or the unrooted
triplet 4t is shown in Figure 1(v). For each tree, the
terminal branch lengths, i.e. the branch lengths leading
to the leaf nodes, have to be strictly positive and the
internal branch lengths have to be non-negative. Our
rooted triplets (Figure 1(i)–(iv)) are said to satisfy the
molecular clock, since the branch lengths of each kt,
where k Œ {0, 1, 2, 3}, satisfy the constraint that the
distance from the root node r to each of the leaf nodes is
equal to kt0 + kt1 with kt1 > 0 and kt0 ≥ 0.

Figure 1
Tree space with three labeled leaves. Space of phylogenetic trees with three labeled leaves {1, 2, 3}. See text for
description.

Page 3 of 19
(page number not for citation purposes)

Algorithms for Molecular Biology 2009, 4:1

http://www.almob.org/content/4/1/1

Likelihood of a tree
Let d denote a homologous set of sequences of length v
with character set U = {a1 , a 2 ,..., a|U|} from n taxa. We
think of d as an n × v matrix with entries from U . We are
interested in estimating the branch lengths and topologies of the tree underlying our observed d. Let bk denote
the number of branches and sk denote the number of
nodes of a tree with a specific topology or branching order
labeled by k. Thus, for a given topology label k, n labeled
leaves and bk many branches, the labeled tree kt is the
k
topology-labeled vector of branch lengths (kt1,..., t b )
k
contained in the topology-labeled tree space k T , i.e.,
k

For an internal node h with descendants s1, s2,..., sℏ,
(
l ha i ) =

∑

{l ( j1 ) ⋅ Pa i , j1 ( k t s1 ) ⋅ l ( j 2 ) ⋅ Pa i , j 2 ( k t s 2 )… l ( j ) ⋅ Pa i , j ( k t s )}.
s
s
s
1

2

j1 ,..., j ∈U

Algorithm 2: Likelihood by post-order traversal
Assuming independence across all v sites we obtain the
likelihood function for the given data d, by multiplying
the site-specific likelihoods
v

ld( k t) =

K

T :=

∪

k

T.

t ).

(2)

arg max l d ( k t ).
k

An explicit model of sequence evolution is prescribed in
order to obtain the likelihood of observing data d at the
leaf nodes as a function of the parameter kt Œ K T for
each topology label k Œ K . Such a model prescribes
Pa i ,a j (t ) , the probability of mutation from a character ai
Œ U to another character aj Œ U in time t. Using such a
transition probability we may compute ℓq(kt), the loglikelihood of the data d at site q Œ {1,..., v} or the q-th
column of d, via the post-order traversal over the labeled
k
tree with branch lengths kt := (kt1, kt2,..., t b ). This amok
unts to the sum-product Algorithm 2 [16] that
associates with each node h Œ {1,..., sk} of kt subtending
ℏ many descendants, a partial likelihood vector,
(a )
(
(
l h := (l ha1 ) , l ha 2 ) ,..., l h |U| ) ∈ R|U| , and specifies the length
of the branch leading to its ancestor as kth.
k

k

input : (i) a labeled tree with branch lengths t := ( t1,
k
k
t2,..., t b ), (ii) transition probability Pa i ,a j (t ) for any
k
ai, aj Œ U , (iii) stationary distribution π(ai) over each
character ai Œ U , (iv) site pattern or data d•, q at site q
output : l d i,q (kt), the likelihood at site q with pattern d•, q
initialize: For a leaf node h with observed character ai =
(
(
dh, q at site q, set l ha i ) = 1 and l ha j ) = 0 for all j ≠ i. For any
internal node h, set lh := (1, 1,...,1).
recurse : compute lh for each sub-terminal node h, then
those of their ancestors recursively to finally compute lr
for the root node r to obtain the likelihood for site q,

∑ (p (a ) ⋅ l
i

a i ∈U

k

The maximum likelihood estimate is a point estimate
(single best guess) of the unknown phylogenetic tree on
the basis of the observed data d and it is

k∈K

l d i,q ( k t ) = l r =

d i ,q (

q =1

T := {k t := ( k t 1 ,..., k t b k ) ∈ R b k : k t i > 0 for terminal branches}.
+

Any subset of the tree space with | K | many topologies in
the topology label set K can be defined as follows:

∏l

(a i )
r ).

t∈ K T

The simplest probability models for character mutation
are continuous time Markov chains with finite state
space U . We introduce three such models employed in
this study next. We only derive the likelihood functions
for the simplest model with just two characters as it is
thought to well-represent the core problems in phylogenetic estimation (see for e.g. [17]).
Posterior density of a tree
The posterior density f• (kt) conditional on data d at tree
k
t is the normalized product of the likelihood ld(kt) and
the prior density p(kt) over a given tree space K T :

f i( k t ) =

l d ( k t )p( k t )
.
k
k
k
∫KT l d ( t )p( t )∂( t )

(3)

We assume a uniform prior density over a large box or a
union of large boxes in a given tree space K T . Typically,
the sides of the box giving the range of branch lengths,
are extremely long, say, [0, 10] or [10-10, 10]. The branch
lengths are measured in units of expected number of
DNA substitutions per site and therefore the support of
our uniform prior density over K T contains the
biologically relevant branch lengths. If K T is a union
of distinct topologies then we let our prior be an equally
weighted finite mixture of uniform densities over large
boxes in each topology. Naturally, other prior densities
are possible especially in the presence of additional
information. We choose at priors for the convenient
interpretation of the target posterior shape
f ( k t ) = f i ( k t )∫K l d ( k t )p( k t )∂( k t ) to be the likelihood
T
function in the absence of prior information beyond a
compact support specification.

Page 4 of 19
(page number not for citation purposes)

Algorithms for Molecular Biology 2009, 4:1

http://www.almob.org/content/4/1/1

Likelihood of a triplet under Cavender-Farris-Neyman (CFN) model
We now describe the simplest model for the evolution of
binary sequences under a symmetric transition matrix
over all branches of a tree. This model has been used by
authors in various fields including molecular biology,
information theory, operations research and statistical
physics; for references see [7,18]. This model is referred
to as the Cavender-Farris-Neyman (CFN) model in
molecular biology, although in other fields it has been
referred to as 'the on-off machine', 'symmetric binary
channel' and the 'symmetric two-state Poisson model'.
Although the relatively tractable CFN model itself is not
popular in applied molecular evolution, the lessons
learned under the CFN model often extend to more
realistic models of DNA mutation (e.g. [17]). Thus, our
first stop is the CFN model.

Model 1 (Cavender-Farris-Neyman (CFN) model) Under
the CFN mutation model, only pyrimidines and purines,
denoted respectively by Y:= {C, T} and R:= {A, G}, are
distinguished as evolutionary states among the four nucleotides
{A, G, C, T}, i.e. U = {Y, R}. Time t is measured by the
expected number of substitutions in this homogeneous continuous time Markov chain with rate matrix:

function over the tree space k T is simplified from (2) as
follows:
v

ld( k t) =

q =1

and transition probability matrix P(t) = eQt :

Thus, the probability that Y mutates to R, or vice versa, in time
t is a(t): (1e-2t)/2. The stationary distribution is uniform on
U , i.e. π(R) = π(Y) = 1/2.
When there are only three taxa, there are five tree
topologies of interest as depicted in Figure 1. There are
23 = 8 possible site patterns, i.e. for each site q Œ {1, 2,...,
v}, the q-th column of the data d, denoted by d•, q, is one
of eight possibilities, numbered 0, 1,...,7 for convenience:
,

1

,

2

,

Y
Y
Y

R
, R
Y

,

3

Y
, Y
R

,

4

R
, Y
Y

,

5

Y
, R
R

,

6

R
, Y
R

k

i

ci

,

(5)

i =0

Consider the unrooted tree-space with a single topology
labeled 4 and three non-negative terminal branch
lengths 4t = (4t1, 4t2, 4t3) Œ R 3 as shown in Figure 1
+
(v). An application of Algorithm 2 to compute the
likelihoods l0(4t), l1(4t),..., l7(4t), as derived in (19)-(25),
reveals symmetry. There are in fact four minimally
sufficient site pattern classes, namely, xxx, xxy, yxx and
xyx, where x and y simply denote distinct characters in
the alphabet set U = {R, Y}. The corresponding
likelihoods are:
1
8
1
l xxy ( 4 t ) := l 2( 4 t ) = l 3( 4 t ) =
8
1
l yxx ( 4 t ) := l 4 ( 4 t ) = l 5( 4 t ) =
8
1
l xyx ( 4 t ) := l6( 4 t ) = l7( 4 t ) =
8
=

(1 + e
(1 + e
(1 − e
(1 − e

−2( 4t 1 + 4t 2 )
−2( 4t 1 + 4t 2 )

+ e −2(
−e

4

t 2 + 4t 3 )

−2( 4t 2 + 4t 3 )

−2( t 1 + t 2 )

+ e −2(

4

−2( 4t 1 + 4t 2 )

− e −2(

4

4

4

4

−e

4

t 1 + 4t 3 )

−2( 4t 1 + 4t 3 )

− e −2(

4

+ e −2(

t2 + t3 )

t 2 + 4t 3 )

+ e −2(

t 1 + 4t 3 )

4

t 1 + 4t 3 )

)
)
)
).

(6)

⎛ 1 − (1 − e −2t ) / 2
(1 − e −2t ) / 2 ⎞
⎟.
P(t ) = ⎜
⎜ (1 − e −2t ) / 2
1 − (1 − e −2t ) / 2 ⎟
⎝
⎠

⎧0
⎪
⎪
⎪
d i,q ∈ ⎨ R
⎪R
⎪
⎪R
⎩

∏ (l ( t))

where li(kt) is the likelihood of the the i-th site pattern as
in (4) and ci is the count of sites with pattern i. In fact,
li(kt) = P(i|kt) is the probability of observing site pattern i
given topology label k and branch lengths t and similarly
ld(kt) = P(d|kt).

l xxx ( 4 t ) := l 0( 4 t ) = l1( 4 t )

⎛ −1 1 ⎞
Q=⎜
⎟,
⎝ 1 −1 ⎠

∏

7

l d i ,q ( k t ) =

7⎫
⎪
⎪
⎪
Y ⎬.
, R⎪
⎪
Y⎪
⎭
,

(4)
Given a multiple sequence alignment data d from 3 taxa
at v homologous sites, i.e. d Œ {Y, R}3 × v, the likelihood

Therefore, the multiple sequence alignment data d from
three taxa evolving under Model 1 can be summarized
by the minimal sufficient site pattern counts
(cxxx, cxxy, cyxx, cxyx):= (c0 + c1, c2 + c3, c4 + c5, c6 + c7),
which simplifies (5) to:
7

v

ld( k t) =

∏
q =1

l d i ,q ( k t ) =

∏ (l ( t))
k

i

i =0

ci

=

∏

(l s ( k t )) c i .

s = xxx , xxy , yxx , xyx

(7)
Note that the probability of our sample space with eight
7
patterns given in (4) is ∑ l i ( 4 t ) = 1 . Our likelihoods
i =0
are half of those in [17] that are prescribed over a sample
space of only four classes of patterns: {0, 1}, {2, 3}, {4,
5} and {6, 7}. This is because we distinguish between
the sample space of data from that of the minimal
sufficient statistics. We compute the rooted topologyspecific likelihood functions, i.e. l(kt) for k Œ {0, 1, 2, 3}
(Figure 1) by substituting the appropriate constraints on
branch lengths in 4 T = R 3 , the space of unrooted
+
triplets.

Page 5 of 19
(page number not for citation purposes)

Algorithms for Molecular Biology 2009, 4:1

http://www.almob.org/content/4/1/1

4lxxx(4t) + 24lxyz(4t) + 12lxxy(4t) + 12lyxx(4t) + 12lyxx(4t) = 1.

Likelihood of a triplet under Jukes-Cantor (JC) model
The r-state symmetric model introduced in [19] is
specified by the r × r rate matrix with equal off-diagonal
entries over an alphabet set U of size r. The stationary
distribution under this model is the uniform distribution
on U . Thus, CFN model is the 2-state symmetric model
over U = {Y, R}. The Jukes-Cantor (JC) model [20] is the
4-state symmetric model over U = {A, C, G, T}. This is
perhaps the simplest model on four characters.

Let cijk denote the number of sites with the site pattern
ijk Œ {xxx, xyz, xxy, yxx, xyx}. Then, under the
assumption of independence across sites, we obtain the
likelihood of a given data d by multiplying the sitespecific likelihoods:

Model 2 (Jukes-Cantor (JC) model) All four nucleotides
form the state space for this mutation model, i.e. U = {A, C,
G, T}. Once again, evolutionary time t is measured by the
expected number of substitutions in the homogeneous
continuous time Markov chain with rate matrix:

Once again, the likelihood of a rooted tree or the star tree
can be obtained from that of the unrooted tree by
substituting the appropriate constraints on branch
lengths in the above equations or by directly applying
Algorithm 2 with the appropriate input tree with its
topology and branch lengths.

⎛ −1 1 / 3 1 / 3 1 / 3 ⎞
⎜
⎟
1 / 3 −1 1 / 3 1 / 3 ⎟
Q=⎜
.
⎜ 1 / 3 1 / 3 −1 1 / 3 ⎟
⎜
⎜ 1 / 3 1 / 3 1 / 3 −1 ⎟
⎟
⎝
⎠
The transition probability matrix P(t) = eQt is also symmetric.
The probability that any given nucleotide mutates to any other
nucleotide in time t is Px, y(t) and that it is found in the same
state is Px, x(t). These transition probabilities are:
a(t ) := Px , y (t ) =

1 1
⎛ 4
− exp ⎜ − t
4 4
⎝ 3

1 3
⎞
⎛ 4
⎟ , b(t ) := Px , x (t ) = + exp ⎜ − t
4 4
⎠
⎝ 3

Consider the three non-negative terminal branch lengths
4
t = (4t1, 4t2, 4t3) Œ R 3 of an unrooted tree 4t of Figure 1
+
(v). An application of Algorithm 2 to compute the
likelihoods of the 64 possible site patterns (see for e.g.
[21-24]), reveals five minimally sufficient site pattern
classes. Let x, y and z simply denote distinct characters
from the alphabet set U = {A, C, G, T} at taxon 1, 2 and
3, respectively. The minimally sufficient site pattern
classes xxx, xyz, xxy, yxx and xyx encode 4, 24, 12, 12 and
12 nucleotide site patterns, respectively. By a computation similar to that in (19)-(25), the likelihoods are:

4

l xyz ( t ) =
l xxy ( 4 t ) =
l xyx ( 4 t ) =
l yxx ( 4 t ) =

3
3
⎞
1⎛
⎜
b( 4 t i ) + 3
a( 4 t i ) ⎟
⎟
4⎜
i =1
⎝ i =1
⎠
1
4
4
4
4
(b( t 1 )a( t 2 )a( t 3 ) + a( t 1
4
1
(b( 4 t 1 )b( 4 t 2 )a( 4 t 3 ) + a( 4 t 1
4
1
(b( 4 t 1 )a( 4 t 2 )b( 4 t 3 ) + a( 4 t 1
4
1
(a( 4 t 1 )b( 4 t 2 )b( 4 t 3 ) + a( 4 t 2
4

∏

∏

c xyz

(l xxy ( 4 t ))

c xxy

(l xyx ( 4 t ))

c xyx

(l yxx ( 4 t ))

c yxx

(l xxx ( 4 t )) c xxx .

Model 3 (Hasegawa-Kishino-Yano (HKY) model) The
Hasegawa-Kishino-Yano or HKY model [25]has all four nucleotides in the state space, i.e. U = {A, C, G, T}. There are five
parameters in this more flexible model. Transitions are changes
within the purine {A, G} or pyrimidine {C, T} state subsets, while
transversions are changes from purine to pyrimidine or from
pyrimidine to purine. In this model, we have a mutational
parameter  that allows for transition:transversion bias and four
additional parameters πA, πC, πG and πT that explicitly control the
stationary distribution. The entries of the rate matrix are:

⎞
⎟.
⎠

The stationary distribution is uniform, i.e. π(A) = π(C) =
π(G) = π(T) = 1/4.

l xxx ( 4 t ) =

l d ( 4 t ) = (l xyz ( 4 t ))

q x ,y

⎧
⎪ kp y
⎪
= ⎨p y
⎪
⎪−
⎩

∑

for transitions
for transversions
z∈U , z ≠ x

q x ,z

if x = y.

The transition probabilities are known analytically for this
model (see for e.g. [[8], p. 203]). We can use these
expressions when evaluating the likelihood of a rooted or
unrooted tree along with the five mutational parameters via
Algorithm 2. For simplicity we set the stationary distribution
parameters to the empirical nucleotide frequencies and  to be
2.0 in this study.
Interval analysis
Let IR denote the set of closed and bounded real
intervals. Let any element of IR be denoted by x: [ x , x ],
where, x ≤ x and x , x Œ R. Next we define arithmetic
over IR .

)(b( 4 t 2 )a( 4 t 3 ) + a( 4 t 2 )(b( 4 t 3 ) + a( 4 t 3 ))))
)a( 4 t 2 )(b( 4 t 3 ) + 2a( 4 t 3 )))
)a( 4 t 3 )(b( 4 t 2 ) + 2a( 4 t 2 )))
)a( 4 t 3 )(b( 4 t 1 ) + 2a( 4 t 1 ))).

Notice that the probability of observing one of the 64
possible site patterns is 1 for any 4t Œ (0, ∞)3 :

Definition 1 (Interval Operation) If the binary operator ⋆
is one of +, -, ×,/, then we define an arithmetic on operands in
IR by
x ⋆ y:= {x ⋆ y : x Œ x, y Œ y},
with the exception that x/y is undefined if 0 Œ y.

Page 6 of 19
(page number not for citation purposes)

Algorithms for Molecular Biology 2009, 4:1

Theorem 1 (Interval arithmetic) Arithmetic on the pair x,
y Œ IR is given by:

x + y = [x + y , x + y ]
x−y

= x × [1 / y , 1 / y ], provided , 0 ∉ y.

and the Hausdorff distance between the boxes x and y in
the metric given by the maximum norm is then

= [min{xy , xy , xy , xy}, max{xy , xy , x y , xy}]

x/y

dist( x , y) = sup{| x − y |,| x − y |},

= [x − y , x − y]

x×y

http://www.almob.org/content/4/1/1

When computing with finite precision, say in floatingpoint arithmetic, directed rounding must be taken into
account (see e.g., [6,10]) to contain the solution. Interval
multiplication is branched into nine cases, on the basis
of the signs of the boundaries of the operands, such that
only one case entails more than two real multiplications.
Therefore, a rigorous computer implementation of an
interval operation mostly requires two directed rounding
floating-point operations. Interval addition and multiplication are both commutative and associate but not
distributive. For example,
[−1, 2] × ([1, 2] + [−2, 1]) = [−1, 2] × [−1, 3] = [−3, 6],
but , [−1, 2] × [1, 2] |[−1, 2] × [−2, 1] = [−2, 4] + [−4, 2] = [−6, 6].

Interval arithmetic satisfies a weaker rule than distributivity called sub-distributivity:
x(y + z) ⊆ xy + xz.
An extremely useful property of interval arithmetic that
is a direct consequence of Definition 1 is summarized by
the following theorem.
Theorem 2 (Fundamental property of interval arithmetic) If x ⊆ x' and y ⊆ y' and ⋆ Œ {+, -, ×,/}, then
x ⋆ y ⊆ x' ⋆ y',
where we require that 0 ∉ y' when ⋆ = /.
Note that an immediate implication of Theorem 2 is that
when x = [x, x] and y = [y, y] are thin intervals, i.e. x = x
= x and y = y = y are real numbers, then x' ⋆ y' will
contain the result of the real arithmetic operation x ⋆ y.
Let x , x Œ Rn be real vectors such that x i ≤ x i , for all i =
1, 2,..., n, then x : [ x , x ] is an interval vector or a box. The
set of all such boxes is IR n . The i-th component of the
box x = (x1,..., xn) is the interval xi = [ x i, x i ] and the
i n t e r v a l e x t e n s i o n o f a s e t D ⊆ Rn i s
ID := {x ∈ IR n : x , x ∈ D} . We write inf x := x for the
lower bound, sup x := x for the upper bound. Let the
maximum norm of a vector x Œ Rn be ∥x∥∞ := maxk |xk|.
Let the vector valued hyper-metric between boxes
x and y be

dist∞ (x, y) = ∥dist(x, y)∥∞.
We can make IR n a metric space by equipping it with
the Hausdorff distance.
Our main motivation for the extension to intervals is to
enclose the range:
range(f; S):= {f(x): x Œ S},
of a real-valued function f : Rn ↦ R over a set S ⊆ Rn.
Except for trivial cases, few tools are available to obtain
the range.
Definition 2 (Directed acyclic graph (DAG) expression
of a function) One can think of the process by which a
function f : Rm ↦ R is computed as the result of a sequence of
recursive operations with the sub-expressions fi of its expression
f where, i = 1,..., n < ∞. This involves the evaluation of the
sub-expression fi at node i with operands s i1 , s i 2 from the subterminal nodes of i given by the directed acyclic graph (DAG)
for f

si =

⎧ f i ( s i1 , s i 2 ) : if node i has 2 sub-terminal nodes s i1 , s i 2
⎪
f i := ⎨ f i ( s i1 )
: if node i has 1 sub-terminal node s i1
⎪
: if node i is a leaf or terminal node, I( x) = x.
⎩ I( s i )

(8)
The leaf or terminal node of the DAG is a constant or a
variable and thus the fi for a leaf i is set equal to the respective
constant or variable. The recursion starts at the leaves and
terminates at the root of the DAG. The DAG for an
elementary f is simply its expression f with n sub-expressions
f1, f2,...,fn:

{ f i}in=1

f n = f ( x),

(9)

where each ⊙fi is computed according to (8).
We look at some DAGs for 0 functions to concretely
illustrate these ideas.
Example 1 Consider the constant zero function f(x) = 0
expressed as (i) f(x) = 0, (ii) f'(x) = x × 0 and (iii) f"(x) = x x. The corresponding DAG expressions are shown in Figure 2.
Definition 3 (The natural interval extension) Consider a
real-valued function f(x): Rn ↦ Rm given by a formula or a
DAG expression f(x). If real constants, variables, and

Page 7 of 19
(page number not for citation purposes)

Algorithms for Molecular Biology 2009, 4:1

http://www.almob.org/content/4/1/1

Figure 2
DAG expression for zero functions. The directed acyclic graph (DAG) expression for the three zero functions: (i) f(x) = 0,
(ii) f'(x) = x × 0 and (iii) f"(x) = x - x.

operations in f(x) are replaced by their interval counterparts,
then one obtains

f( x) : IR n

IR m .

f(x) is known as the natural interval extension of the
expression f(x) for f(x). This extension is well-defined if we
do not run into division by zero.
Although the three distinct expressions f(x), f'(x) and
f"(x) of the real function f: R ↦ R of Example 1 are
equivalent upon evaluation in the reals, their respective
interval extensions f(x) = [0, 0], f'(x) = x × [0, 0], and
f"(x) = x - x are not. For instance, if x = [1, 2],
f ([1, 2]) = [0, 0],
f ′([1, 2]) = [1, 2] × [0, 0] = [min{1 × 0, 1 × 0, 2 × 0, 2 × 0}, max{1 × 0, 1 × 0, 2 × 0, 2 × 0}] = [0, 0]
f ′′([1, 2]) = [1, 2] − [1, 2] = [1 − 2, 2 − 1] = [−1, 1],

and in general for any x : [ x , x ] Œ IR ,
f ([ x , x ]) = [0, 0],
f ′([ x , x ]) = [ x , x ] × [0, 0] = [min{x × 0, x × 0, x × 0, x × 0}, max{x × 0, x × 0, x × 0, x × 0}] = [0, 0]
f ′′([ x , x ]) = [ x , x ] − [ x , x ] = [ x − x , x − x] ≠ [0, 0], unless x = x .

Thus, f(x) = f'(x) ≠ f"(x) for any x Œ IR , albeit f(x) =
f'(x) = f"(x) for any x Œ R.

Such functions have well-defined interval extensions that
satisfy inclusion isotony and exact range enclosure, i.e.
range(f; x) = f(x). Consider the following definitions for
the interval extensions for some monotone functions in
S with x Œ IR ,

exp( x)
= [exp( x ), exp( x )]
arctan( x) = [arctan( x ), arctan( x )]
( x)
log( x)

= [ ( x ), ( x )]
= [log( x), log( x )]

if 0 ≤ x
if 0 < x ,

and a piece-wise monotone function in S ; with ℤ+ and
ℤ- representing the set of positive and negative integers,
respectively. Let the mignitude of an interval x be the
number x = min{|x|:x Œ x} and the absolute value of x
be the number |x| = max{|x|:x Œ x} = sup{- x , x }. Then,
the interval-extended power function that plays a basic
role in product likelihood functions is:

⎧ [x n , x n]
⎪
⎪ [〈 x 〉 n ,| x |n ]
n
x =⎨
]
⎪ [1, 1]
⎪
−n
⎩ [1 / x , 1 / x]

: if n ∈ Z + is odd,
: if n ∈ Z + is even,
: if n = 0,
: if n ∈ Z − ; 0 ∉ x.

Theorem 3 (Interval rational functions) Consider the
rational function f(x) = p(x)/q(x), where p and q are
polynomials. Let f be the natural interval extension of its DAG
expression f such that f(y) is well-defined for some y Œ IR
and let x, x' Œ IR . Then we have

Definition 5 (Elementary functions) A real-valued
function that can be expressed as a finite combination of
constants, variables, arithmetic operations, standard functions
and compositions is called an elementary function. The set of
all such elementary functions is referred to as E .

(i) Inclusion isotony : ∀x ⊆ x′ ⊆ y ⇒ f ( x) ⊆ f ( x′), and
(ii) Range enclosure : ∀x ⊆ y ⇒ range( f ; x) ⊆ f ( x).

Example 2 (Probability of the pattern xxx under CFN
star tree 0t) The trifurcating star-tree 0t := (0t1) has topology
label 0 and common branch length parameter 0t1 as shown in
Figure 1(i). Either a direct application of Algorithm 2 with
input as 0t := (0t1) or a substitution of 0t1 for 4t1, 4t2 and 4t3
in (6), yields the likelihood for pattern xxx as:

Definition 4 (Standard functions) Piece-wise monotone
functions, including exponential, logarithm, rational power,
absolute value, and trigonometric functions, constitute the set
of standard functions
x

S = {a , logb(x), x
arcsin(x),...}.

p/q

, |x|, sin(x), cos(x), tan(x), sinh(x),...

l xxx ( 0 t ) = (1 + 3e −4(

0

t1 )

) / 8.

Page 8 of 19
(page number not for citation purposes)

Algorithms for Molecular Biology 2009, 4:1

http://www.almob.org/content/4/1/1

Figure 3
DAG expression for probability of the pattern xxx under a CFN star tree. The elementary function
0
10
l 0( 0 t ) = (1 + 3e −4( t 1 ) ) / 8. can be obtained from the terminus 0⊙f10 of the recursion { f i}i=1 over the sub-expressions f1,...,
f10 in the above directed acyclic graph (DAG) expression of l0( t). Note that the leaf nodes are constants (s2, s5, s7 and s9) or
variables (s1).
The probability of the pattern xxx under CFN star tree 0t given
by lxxx(0t) with the corresponding DAG expression shown in
Figure 3 is an elementary function.
It would be convenient if guaranteed enclosures of the
range of an elementary f can be obtained by the natural
interval extension f of one of its expressions f. The
following Theorem 4 is the work-horse of interval Monte
Carlo algorithms.
Theorem 4 (The fundamental theorem of interval
analysis) Consider any elementary function f Œ E with
expression f. Let f : y ↦ IR be its natural interval extension
such that f(y) is well-defined for some y Œ IR and let x,
x' Œ IR . Then we have
(i) Inclusion isotony : ∀x ⊆ x′ ⊆ y ⇒ f ( x) ⊆ f ( x′),
and
(ii) Range enclosure : ∀x ⊆ y ⇒ range( f ; x) ⊆ f ( x).

The fundamental implication of the above theorem is
that it allows us to enclose the range of any elementary
function and thereby produces an upper bound for the
global maximum and a lower bound for the global
minimum over any compact subset of the domain upon
which the function is well-defined. This is the workhorse for rigorously constructing an envelope for
rejection sampling.
Unlike the natural interval extension of an f Œ S that
produces exact range enclosures, the natural interval
extension f(x) of an f Œ E often overestimates range(f;
x), but can be shown under mild conditions to linearly
approach the range as the maximal width of the box x
goes to zero. This implies that a partition of x into
smaller boxes {x(1),..., x(m)} gives better enclosures of
m
range(f; x) through the union ∪ f( x (i) ) as illustrated
i =1
in Figure 4. Next we make the above statements precise
in terms of the width and radius of a box x defined by wid
x := x - x and rad x := ( x - x )/2, respectively.

Figure 4
Adaptive range enclosure of the posterior density
over the star-tree space. Range enclosure of the loglikelihood (white line) for the human, chimpanzee and gorilla
mitochondrial sequence data [27] analyzed in [17], under the
CFN model with cxxx = 762 and v = 895 over star-trees, via
its interval extension linearly tightens with the mesh. One
hundred samples (+) from the MRS and the maximum
likelihood estimate (red dot) are shown.

Definition 6 A function f: D ↦ R is Lipschitz if there exists
a Lipschitz constant K such that, for all x, y Œ D , we have
|f(x) - f(y)| ≤ K|x - y|. We define E L to be the set of
elementary functions whose sub-expressions fi, i = 1,..., n at
the nodes of its corresponding DAG f are all Lipschitz:
E L := { f ∈ E : each sub-expression f i in the DAG expression f for f is Lispschitz}.

Theorem 5 (Range enclosure tightens linearly with
mesh) Consider a function f : D ↦ Rwith f Œ E L . Let f be
an inclusion isotonic interval extension of the DAG expression
f of f such that f (x) is well-defined for some x Œ IR . Then

Page 9 of 19
(page number not for citation purposes)

Algorithms for Molecular Biology 2009, 4:1

http://www.almob.org/content/4/1/1

there exists a positive real number K, depending on f and x,
k
such that if x = ∪ x (i) , then
i =1

Thus, under our conveniently chosen uniform prior, the target
posterior shape (without the normalizing constant) is simply
the likelihood function, i.e.

k

∪ f (x

range( f ; x) ⊆

(i )

) ⊆ f ( x),

∫

f ( 0 t ) = f i( 0 t )

i =1

and

⎛
rad ⎜
⎜
⎝

k

∪ f(x

(i )

i =1

⎞
) ⎟ ≤ rad (range( f ; x)) + K max rad( x (i) ).
i =1,...,k
⎟
⎠

10

0

l d ( 0 t )∂( 0 t ) = l d ( 0 t ).

Observe that the minimal sufficient statistics over 0 T are the
number of sites with the same character cxxx := c0 + c1 and
the total number of sites v. Let the natural interval extension of
the DAG expression for the posterior shape f(0t): 0 T ↦ R be:

f( 0 t) : 0 TT

IR.

Thus, f maps an interval t in the tree space 0 T to an interval
in IR that encloses the target shape or likelihood of 0t.
0

Likelihood of a box of trees
The likelihood function (2) over trees with a DAG
expression that is directly or indirectly obtained via
Algorithm 2 has a natural interval extension over boxes
of trees [5,26]. This interval extension of the likelihood
function allows us to produce rigorous enclosures of the
likelihood over a box in the tree space. Next we give a
concrete example of the natural interval extension of the
likelihood function over an interval of trees 0t in the startree space 0 T . The same ideas extend to any labeled box
of trees kt when the number of branch lengths is greater
than one and more generally to a finite union of labeled
boxes with possibly distinct labels.

Example 3 (Posterior density over the CFN star-tree
space 0 T ) The trifurcating star-tree 0t := (0t1) has topology
label 0 and common branch length 0t1 > 0. Either a direct
application of Algorithm 2 with input triplet 0t or a
substitution of 4t1, 4t2 and 4t3 in (6) by 0t1 yields the
following 0 T -specific likelihoods:
l 0( 0 t ) = l1( 0 t ) = (1 + 3e −4(
0

0

0

0

0

0

l 2( t ) = l 3( t ) = l 4 ( t ) = l 5( t ) = l6( t ) = l7( t ) =

(1 − e

0

t1 )

0

−4( t 1 )

) / 8,
) / 8.

(10)
Therefore, on the basis of (4), (5), (6) and (7), the likelihood
of the data at the star-tree 0t Œ 0 T is

∏ (l ( t)) = ( (1 + 3e
7

ld( 0 t) =

0

i

ci

i =0

=

( (1 + 3e

−4( 0t 1 )

)/8

−4( 0t 1 )

)/8

) ( (1 − e
c 0 + c1

) ( (1 − e
)/8 )
c 0 + c1

−4( 0t 1 )

−4( 0t 1 )

v −(c 0 +c1 )

)/8

0

t = arg max f ( 0 t ) = (0.055205).
0

t∈ 0 T

Moore rejection sampler (MRS)
Moore rejection sampler (MRS) is an auto-validating
rejection sampler (RS). MRS is said to be auto-validating
because it automatically obtains a proposal g that is easy
ˆ
to simulate from, and an envelope g that is guaranteed
to satisfy the envelope condition (1). MRS can produce
independent samples from any target shape f whose
DAG expression f has a well-defined natural interval
extension f over a compact domain T . In summary, the
defining characteristics and notations of MRS are:
Compact domain
Target shape

T = [t , t ]
f (t ) : T
R

Target integral

N f :=

Target density
DAG expression of f
Interval extension of f
Envelope function

f (t ) := (N f ) −1 f (t ) : T
f (t ) : T
R
f (t) : IT
IR
ˆ
g(t ) : T
R

Envelope integral

N g :=
ˆ

Proposal density

ˆ
g(t ) := (N g ) −1 g(t ) : T
ˆ

Acceptance probability

)∑

ˆ
A( g) = N f / N g
ˆ

Partitionof T

T := {t (1) , t (2) ,..., t (|T|)}.

7

c
i =2 i

,

(11)
the posterior density (3) based on a uniform prior p(0t1) =
1/10 over 0 T = (0, 10] is

ld( 0 t)
f i( 0 t ) =
.
0
10 0
∫0 l d ( t )∂( t )

For the human, chimpanzee and gorilla mitochondrial
sequence data [27]analyzed in [17], cxxx = 762 and v =
895. Figure 4 shows log(f(0t)) or the log-likelihood function
for this data set as the white line. Evaluations of its interval
extension over partitions by 3, 7 and 19 intervals are depicted
by colored rectangles in Figure 4. Notice how the range
enclosure by the interval extension of the log-likelihood
function, our target shape, tightens with domain refinement
as per Theorem 5. The maximum likelihood estimate derived
in [17](the red dot in Figure 4) is

(12)

i

∫

T

∫

T

f (t )dt
R

ˆ
g(t )dt
R

Page 10 of 19
(page number not for citation purposes)

Algorithms for Molecular Biology 2009, 4:1

http://www.almob.org/content/4/1/1

Suppose f is an elementary function and its DAG
expression f has a well-defined interval extension f on
T . If T := {t (1) , t (2) ,..., t (|T|)} is a finite partition of T ,
then by Theorem 4 we can enclose range(f; t(i)), i.e. the
range of f over the i-th element of T , with the interval
extension f of f:
range( f ; t (i) ) ⊆ f (t (i) ) := [f (t (i) ), f (t (i) )], ∀i ∈ {1, 2,...,| T |}.

(13)
For a given partition T , we can construct a partitionspecific envelope function:
|T|

ˆ
g T (t ) =

∑ f (t
i =1

(i )

⎧ 1 if t ∈ t (i)
⎪
)1{t∈t (i )}, 1{t∈t (i )} = ⎨
i
⎪
⎩ 0 otherwise.
(14)

The necessary envelope condition (1) is satisfied by
ˆ
g T (t) because of (13). We can obtain the corresponding
proposal g T (t) as a normalized simple function over T :

(

g T (t ) = N g T

)

−1

(

g T (t ) = N g T

|T|

) ∑ f (t
−1

i =1

(i)

)1{t∈t (i )},
(15)

where the normalizing constant N g T := ∑
⋅ f (t (i) ))
ˆ
n
and vol t := ∏ wid t i is the volume of the box t. The
i =1
volume of an interval x is simply its width, i.e. vol x = wid x,
if x Œ IR . Now, we have all the ingredients to perform
a more efficient, partition-specific, auto-validating von
Neumann rejection sampling or simply Moore rejection
sampling.
|T|
( vol t (i)
i =1

identically distributed samples from the target posterior
density. Note how the acceptance probability (ratio of the
area below the target shape to that below the envelope)
increases with refinement.
Theorem 6 shows that Moore rejection sampler (MRS)
indeed produces independent samples from the desired
target and Theorem 7 describes the asymptotics of the
acceptance probability as the partition of the domain is
refined. Proofs for both Theorems are included in the
Appendix for completeness.
Theorem 6 Suppose that the DAG expression f of the
target shape f has a well-defined natural interval extension
f over T ∈ IR n . If T is generated according to Algorithm 1,
and if the the envelope function g T (t) and the proposal
ˆ
density g T (t) are given by (14) and (15), respectively,
then T is distributed according to the target density
f• : T ↦ R.
Next we bound the partition-specific acceptance probability A(T) : A( g T ) for this sampler. For simplicity, let
the domain T of the target shape f be an interval. Due to
the linearity of the integral operator and (13),
Nf

:=
=
∈
=

∫ f (t)dt
∑ ∫ f (t)dt
∑ (wid (τ ) ⋅ f (τ ))
[∑ (wid (τ ) ⋅ f (τ )),∑
T

|T|

i =1 τ ( i )

|T|

(i )

i =1
|T|

(i )

(i )

(i )

i =1

|T|
i =1

(wid (τ (i) ) ⋅ f (τ (i) ))].

Therefore,
Before making formal statements about our sampler let
us gain geometric insight into the sampler from Example 3
and Figure 4. The upper boundaries of rectangles of a given
color, depicting a simple function in Figure 4, is a partitionspecific envelope function (14) for the logarithm of the
posterior shape or the log-likelihood function of Example 3
over the prior-specified support [10-10, 10] ⊂ 0 T . In Figure 4
only a small interval about the maximum likelihood
estimate (red dot) that contains the posterior samples
(gray '+' markers) is depicted since the likelihood falls
sharply outside this range. Normalization of the envelope
gives the corresponding proposal function (15). As the
refinement of the domain proceeds through adaptive
bisections (described later), the partition size increases. We
show partitions of size 3,7 and 19 over an interval containing
the posterior samples. These samples were obtained from the
partition with 19 intervals. Each of the corresponding
envelope functions (upper boundaries of rectangles of a
given color) can be used to draw independent and

A(T) := A( g T ) =

Nf
=
NgT

Nf
|T|
(i)
(i)
∑i =1 (wid (t )⋅ f (t ))

≥

|T|
(i)
(i)
∑i =1 (wid (t )⋅ f (t ))
.
|T|
(i)
(i)
∑i =1 (wid (t )⋅ f (t ))

(16)
If f Œ E L , the Lipschitz class of elementary functions
(Definition 6), then we might expect the enclosure
of Nf to be proportional to the mesh
mesh w := max i∈{1,...,T} wid (t (i) ) of the partition T .
Theorem 7 Let U W be the uniform partition of

(t − t )
W

w
T =[

=

(i
τ W)

= [ t + (i − 1)w , t + iw], i = 1,..., W

, t]

(i
U W = {τ W) , i = 1,..., W}.
into W intervals each of width w

Page 11 of 19
(page number not for citation purposes)

Algorithms for Molecular Biology 2009, 4:1

http://www.almob.org/content/4/1/1

(t − t )
W

g T (t (i) ) =

w

=

(i
t W)

= [ t + (i − 1)w , t + iw], i = 1,..., W

UW

(i
= {t W) , i = 1,..., W}.

A( U W ) = 1 − O(1 / W )
Theorem 7 shows that if f Œ E L and U W is a uniform
partition of T into W intervals, then the acceptance
probability A( U W ) = 1 − O(1 / W ) . Thus, the acceptance
probability approaches 1 at a rate that is no slower than
linearly with the mesh.
Prioritized partitions and pre-processed proposals
We studied the efficiency of uniform partitions for their
mathematical tractability. In practice, we may further
increase the acceptance probability for a given partition
size by adaptively partitioning T . In our context,
adaptive means the possible exploitation of any current
information about the target. We can refine the current
partition Ta and obtain a finer partition Ta ′ with an
additional box by bisecting a box t (*) Œ Ta along the
midpoint of its side with the maximal width into a left
box t (∗) and a right box t (∗) . There are several ways to
L
R
choose a box t (*) Œ Ta for bisection. For instance, a
relatively optimal choice is

t ( i ) ∈Ta

(

)

(

)

(18)

2. Sample a point t uniformly at random from the box t(i).

and let f Œ E L , then

t (*) = arg max vol (t (i) ) ⋅ wid (f (t (i) ) .

vol t (i)f (t (i))
, t (i ) ∈ T ,
|T|
(i)f (t (i))
∑i =1 vol t

(17)

We employ a priority queue to conduct sequential
refinements of T under this partitioning scheme. This
approach avoids the exhaustive argmax computations to
obtain the t(*) for bisection at each refinement step.
Thus, the current partition is represented by a queue of
boxes that are prioritized in descending order by the the
priority function vol (t(i)) · wid (f(t(i)) in (17). Therefore, the box with the largest uncertainty in the enclosure
of the integral over it gets bisected first. There are several
ways to decide when to stop refining the partition. A
simple strategy is to stop when the number of boxes
reaches a number that is well within the memory
constraints of the computer, say 106, or when the
lower bound of the acceptance probability given by
(16) is above a desired threshold, say 0.1.
Once we have a partition T of T , we can sample t from
the proposal density g T given by (15) in two steps:
1. Sample a box t(i) Œ T according to the discrete
distribution:

Sampling from large discrete distributions (with million
states or more) can be made faster by pre-processing the
probabilities and saving the result in some convenient
look-up table. This basic idea [28] allows samples to be
drawn rapidly. We employ an efficient pre-processing
strategy known as the Alias Method [4] that allows
samples to be drawn in constant time even for very large
discrete distributions as implemented in the GNU
Scientific Library [29]. We also minimize the number
of evaluations of the target shape f by saving the boxspecific computations of f (t(i)) and f (t(i)) and exploiting the so-called "squeeze principle", i.e. immediately
accepting those points proposed in the box t(i) that fall
below f (t(i)) when uniformly stretched toward f (t(i)).
Thus, by means of priority queues and look-up tables we
can efficiently manage our adaptive partitioning of the
domain for envelope construction, and rapidly draw
samples from the proposal distribution. Our sampler
class MRSampler implemented in MRS 0.1.2, a C++ class
library for statistical set processing, builds on C-XSC 2.0,
a C++ class library for extended scientific computing
using interval methods [30]. All computations were
done on a 2.8 GHz Pentium IV machine with 1 GB RAM.
Having given theoretical and practical considerations to
our Moore rejection sampler, we are ready to draw
samples from various targets over small tree spaces.

Results
The natural interval extension of the likelihood function
over labeled boxes in the tree space allows us to employ
the Moore rejection sampler to rigorously draw independent and identically distributed samples from the
posterior distribution over a compact box in the tree
space given by our prior distribution. We draw samples
from the posterior distribution based on two mitochondrial DNA data sets and use these samples (i) to estimate
the posterior probabilities of each of the three rooted
topologies, (ii) to conduct a nonparametric test of rate
homogeneity between protein-coding and tRNA-coding
sites and (iii) to estimate the human-neanderthal
divergence time.
Human, chimpanzee and gorilla
We revisit the data from a segment of the mitochondrial
DNA of human, chimpanzee and gorilla [27] that was
analyzed under the CFN model of DNA mutation
(Model 1) within a point estimation setting [17]. The
sufficient statistics of pattern counts for this data with

Page 12 of 19
(page number not for citation purposes)

Algorithms for Molecular Biology 2009, 4:1

http://www.almob.org/content/4/1/1

total number of sites v = 895 under the CFN model over
the space of all three-leaved phylogenetic trees are:
(cxxx, cxxy, cyxx, cxyx) = (762, 54, 41, 38)
Let human, chimpanzee and gorilla be denoted by leaf
labels 1, 2 and 3, or H, C and G, respectively. Let the set
of rooted tree labels corresponding to (ii),(iii) and (iv)
of Figure 1 be K = {1, 2, 3}. The maximum likelihood
estimate over K T := 1 T ∪ 2 T ∪ 3 T , the rooted and
clocked three-leaved phylogenetic tree space, is derived
in [17] as
1

These point estimates are in agreement with estimates
obtained in [31,32] through quadrature routines in
Mathematica. The first 10,000 of these samples are
shown in Figure 5 upon transforming the rooted and
clocked trees, it := (it0, it1), i Œ {1, 2, 3}, into constrained
unrooted trees, 4t := (4t1, 4t2, 4t3), according to Table 1.

t := (1t 0 , 1t 1 ) = arg max f ( i t 0 , i t 1 ) = (0.010036, 0.048559).
( it 0 , it 1 )∈ K T

Recall that due to our flat priors, our posterior shape f
(it):= f(it0, it1) with i Œ K = {1, 2, 3} is our likelihood
function over K T . Now, suppose b1 t (1) , b 2 t (2) ,..., b n t (n)
are n independent and identically distributed samples
from the posterior density f•. over K T . We can obtain
asymptotically consistent estimates of the posterior
probabilities of 1 T , 2 T and 3 T from Monte Carlo
integration of the indicator function of each of the three
topology labels using
j

P n :=

1
n

n

∑1

{b i = j}(

bi

P

t ) → j P :=

i =1

∫

K

T

⎧ 1 if b i = j
1{i = j}( i t ) f i ( i t )∂( i t ), 1{bi = j}( bi t (i) ) = ⎨
⎩ 0 otherwise.

The 95% confidence interval for jP, based on asymptotic
normality of the Monte Carlo estimator, is
j

P n ± 1.96

j

P n (1 − j P n ) / n .

Point estimate and a symmetric 95% confidence interval
for the posterior probability of each of the three
topologies from n = 106 posterior samples are
1

P 10 6 = 0.8875 ± 0.0006,

2

P 10 6 = 0.0646 ± 0.0005,

3

P 10 6 = 0.0479 ± 0.0004.
0

Figure 5
Posterior samples from the rooted tree space of
human, chimpanzee and gorilla. Ten thousand
independent and identically distributed posterior samples
from the rooted and clocked binary tree space of human,
chimpanzee and gorilla with topology label set {1 ∪ 2 ∪ 3}
(see Figure 1(ii),(iii),(iv)) on the basis of mitochondrial data
[27] summarized by (cxxx, cxxy, cyxx, cxyx) = (762, 54, 41, 38)
under the Cavender-Farris-Neyman model (blue ∪ red ∪
green dots, respectively) are depicted.

Table 1: Rooted triplets as constrained unrooted triplets

Rooted and Clocked Trees, it := (it0, it1), i Œ {1, 2, 3}
Labeled Tree
1

1

1

t := ( t0, t1)
t := (2t0, 2t1)
3
t := (3t0, 3t1)
2

Unrooted Trees 4t := (4t1, 4t2, 4t3)

Newick Representation of it
1

1

1

1

4

1

((H: t1, C: t1): t0, G: t0 + t1))
((C:2t1, G:2t1):2t0, H:2t0 + 2t1))
((H:3t1, G:3t1):3t0, C:3t0 + 3t1))

t1

4

t1
t 1 + 2 t 0 + 2t 0
3
t1

1

1

2

t1
t1
3
t1 + 3t0 + 3t0
2

4

t2
1

t3

1

t1 + t0 + 1 t0
2
t1
3
t1

Any labeled, rooted and clocked tree with three leaves can be represented as a constrained unrooted tree according to the tabulated transformation.

Page 13 of 19
(page number not for citation purposes)

Algorithms for Molecular Biology 2009, 4:1

http://www.almob.org/content/4/1/1

Obtaining confidence intervals from dependent MCMC
samples requires nontrivial computations for the burnin period and the thinning rate [1]. These are not
readily available for phylogenetic MCMC samplers.
Thus, the independent and identically distributed
samples from our rejection sampler has the advantage
of producing valid confidence intervals for our integrals
of interest. The point estimate of the posterior
mean E(1 T ) := ∫1 1 t f (1 t )∂(1 t ) for topology label 1 is
T
(0.010863, 0.048994). This posterior mean is close to
(0.010036, 0.048559), the mode of our target shape or
the maximum likelihood estimate derived in [17].
Chimpanzee, gorilla and orangutan
We focus here on the 895 bp long homologous segment
of mitochondrial DNA from chimpanzee, gorilla and
orangutan [27]. This gives us a greater phylogenetic
depth than the human, chimpanzee and gorilla
sequences that were just analyzed. These sequences
encode the genes for three transfer RNAs and parts of
two proteins. Under the assumption of independence
across sites, the sufficient statistics, under the JC model
of DNA mutation (Model 2) over triplets, are given in
Table 2 for all of the data as well as a partition of the
data into tRNA-coding and protein-coding sites.

Ten thousand independent and identically distributed
samples were drawn in 942 CPU seconds from the
posterior distribution over JC triplets, i.e. unrooted trees
with three edges corresponding to the three primates.
Figure 6 shows these samples (blue dots) scattered about
the verified global maximum likelihood estimate (MLE)
of the triplet obtained in [5,26] and subsequently
confirmed algebraically in [23]. We also drew ten
thousand independent and identically distributed samples from the posterior based on the 198 tRNA-coding
DNA sites (green dots in Figure 6) as well as from that
based on the remaining 697 protein-coding sites (red
dots in Figure 6). The former posterior samples,
corresponding to the tRNA-coding sites, are more
dispersed than the posterior samples based on the entire
sequence. This is due to the smaller number of tRNAcoding sites making the posterior less concentrated.
Table 2: Minimal sufficient statistics for the chimpanzee, gorilla
and orangutan data

Site type
All
tRNA-coding
protein-coding

v

cxxx

cxxy

cyxx

cxyx

cxyz

895
198
697

700
173
527

100
13
87

46
7
39

42
3
39

7
2
5

The minimal sufficient statistics under the JC model for all 895 sites, 198
tRNA-coding sites and 697 protein-coding sites based on the
homologous segment of mitochondrial DNA from chimpanzee, gorilla
and orangutan [27].

Figure 6
Posterior samples from the unrooted tree space of
chimpanzee, gorilla and orangutan. Ten thousand
Moore rejection samples from the posterior distribution
over the three branch lengths of the unrooted tree space of
chimpanzee, gorilla and orangutan based on their
homologous mitochondrial DNA sequence of length 895
base pairs (blue dots), the tRNA-coding sequence with 198
base pairs (green dots) and the protein-coding sequence with
697 base pairs (red dots). The verified maximum likelihood
estimate is the large black dot within the blue dots.

Moreover, the cluster of samples from the posterior
based on tRNA-coding sites seem to be farther away from
that based on protein-coding sites. Such a clustering of
two sets of posterior samples is a signal of mutational
rate heterogeneity between the two types of sites.
Hotelling's trace statistics, being a natural measure of
distance between two clusters of points, can be used as a
test statistic to determine the significance of the observed
test statistic. On the basis of 100 random permutations of
the sites, we obtain the null distribution of Hotelling's
trace statistics. We were able to reject the null hypothesis
of rate homogeneity between the posterior samples based
on the tRNA-coding sites and that based on the proteincoding sites at the 10% significance level using this
permutation test (P-value = 0.06). Any biological interpretation of this test must be done cautiously since the JC

Page 14 of 19
(page number not for citation purposes)

Algorithms for Molecular Biology 2009, 4:1

http://www.almob.org/content/4/1/1

model employed here forbids any transition:transversion
bias that is reportedly relevant for this data [27].
Neanderthal, human and chimpanzee
We used the 15 site patterns and their counts in Table 3
to infer the human-neanderthal divergence time. These
counts are obtained from a multiple sequence alignment
of the data made available in [33]. Our alignment
procedure is more robust at the ends of each locus than
that of [33]. We do an ordered concatenation of all the
loci for each species prior to a multiple sequence
alignment. The alignment was further edited by hand
to obtain the locus-specific alignments. Under the
assumption of independence across sites, the sufficient
statistics, under any Markov model of DNA mutation, is
the set of distinct site patterns and their respective
counts. They are given in Table 3 for this data set.

We drew 10,000 samples that were independently and
identically distributed from each of three posterior
densities; (i) over the space of unrooted triplets under
the JC model in 312 CPU seconds, (ii) over the clocked
and rooted triplets under the JC model in 375 CPU
seconds and (iii) over the clocked and rooted triplets
under the HKY model in 1.2 CPU hours. In the HKY
model we used the empirical nucleotide frequencies
from the data (π(T) = 0.2588, π(C) = 0.2571, π(A) =
0.2916, π(G) = 0.1925) and a hominid-specific transition:transversion rate of 2.0. Unlike the JC model with
five sufficient statistics (cxxx, cxxy, cyxx, cxyx, cxyz) = (2343,
56, 2, 4, 0), all 15 distinct site patterns are required for
the likelihood computations under the HKY model and
this is reflected in its longer CPU time. Both models gave
similar posterior samples over rooted triplets, as shown
in Figure 7.
Table 3: Minimal sufficient statistics for the neanderthal, human
and chimpanzee data

Site
:0000000
Pattern
:1234567
.....................
neanderthal
:atcgatcg
human
:atcgatcg
chimpanzee
:atcggcta
.....................
site
:6664111
pattern
:8005544
counts
:5530

00111111
89012345
ttgacaa
tcagtag
ataactg
12211111
0

Site patterns and their counts from a multiple sequence alignment of the
whole mitochondrial genome shotgun sequence (gi|115069275) of a
neanderthal fossil Vi-80, from Vindija cave, Croatia [33], and its
homologous sequence in a human (gi|13273200) and a chimpanzee (gi|
1262390). The first column, (01.aaa.685)T, expresses that there are 685
sites with nucleotide a in all three species,..., and the fif-teenth column,
(15.agg.1)T, expresses that there is 1 site with nucleotide g in human and
chimpanzee and nucleotide a in neanderthal.

Figure 7
Posterior samples from the unrooted tree space of
neanderthal, human and chimpanzee. Ten thousand
Moore rejection samples each from the posterior
distribution over the three branch lengths of the unrooted
tree space of neanderthal, human and chimpanzee under the
JC model (blue dots) and the HKY model (red dots).

We transformed the three posterior distributions over the
triplet spaces; (i) unrooted JC triplets that were rooted
using the mid-point rooting method, (ii) rooted JC
triplets and (iii) rooted HKY triplets, respectively, into
three posterior distributions over the human-neanderthal divergence time relative to the human-chimp
divergence time. The corresponding posterior quantiles
({5%, 50%, 95%}) for the human-neanderthal divergence time in units of human-chimp divergence time are
{0.0643, 0.125, 0.214}, {0.0694, 0.142, 0.263} and
{0.0682, 0.143, 0.268}, respectively. We constrained the
neanderthal lineage to be a fraction of the human
lineage in branch length in order to estimate the age of
the neanderthal fossil from the rooted HKY triplets. The
posterior quantiles of the fossil date in units of humanchimp divergence is {0.00685, 0.0666, 0.195}. The
estimate of 38; 310 years based on carbon-14 accelerator
mass spectrometry [33] is within our [5%, 95%] posterior quantile interval for the fossil date, provided the
human-chimp divergence estimate ranges in [196103,
5.6 × 106]. Thus, reasonable bounds for the humanchimp divergence are 4 × 106 and 5.6 × 106 years, under
the assumption that 4 × 106 is an acceptable lowerbound. Based on these two calendar year estimates, we
transformed the posterior quantiles of the humanneanderthal divergence times from the rooted HKY
triplets into {272680, 571124, 1073375} and
{381752, 799574, 1502724} years, respectively. Our
[5%, 95%] posterior intervals contain the interval
estimate of [461000, 825000] years reported in [33].
However, our confidence intervals are from perfectly
independent samples from the posterior and account for
the finite number of neanderthal sites that were
successfully sequenced, unlike those obtained on the

Page 15 of 19
(page number not for citation purposes)

Algorithms for Molecular Biology 2009, 4:1

basis of a bootstrap of site patterns [34] or heuristic
MCMC [1]. Unfortunately, our human-neanderthal
divergence estimates are overestimates as they ignore
the non-negligible time to coalescence of the human and
neanderthal homologs within the human-neanderthal
ancestral population. Improvements to our estimates
based on the other 310 human and 4 chimpanzee
homologs reported in [33] may be possible with more
sophisticated models of populations within a phylogeny
and needs further investigation.
Chimpanzee, gorilla, orangutan and gibbon
We were able to draw samples from JC quartets on the basis
of the mitochondrial DNA of chimpanzee, gorilla, orangutan and gibbon [27]. The data for all four primates can be
summarized by 61 distinct site patterns [5]. Now, the
problem is more challenging because there are three distinct
tree topologies in the unrooted, bifurcating, quartet tree
space, and each of these topologies has five edges. Thus, the
domain of quartets is a piecewise Euclidean space that arises
from a fusion of 3 distinct five dimensional orthants. Since
the post-order traversals (Algorithm 2) specifying the
likelihood function are topology-specific, we extended the
likelihood over a compact box of quartets in a topologyspecific manner. The computational time was about a day
and a half to draw 10000 samples from the quartet target
due to low acceptance probability of the naive likelihood
function based on the 61 distinct site patterns. All the
samples had the topology which grouped Chimp and
Gorilla together, i.e. ((chimpanzee, gorilla), (orangutan,
gibbon)). The samples (results not shown) were again
scattered about the verified global MLE of the quartet [5].
This quartet likelihood function has an elaborate DAG with
numerous operations. When the data got compressed into
sufficient statistics, the efficiency increased tremendously
(e.g. for triplets the efficiency increases by a factor of 3.7).
This is due to the number of leaf nodes in the target DAG,
which encode the distinct site patterns of the observed data
into the likelihood function, getting reduced from 29 to 5
for the triplet target and from 61 to 15 for the quartet target
[24].

Discussion
Interval methods provide for a rigorous sampling from
posterior target densities over small phylogenetic tree
spaces. When one substitutes conventional floatingpoint arithmetic for real arithmetic in a computer and
uses discrete lattices to construct the envelope and/or
proposal, it is generally not possible to guarantee the
envelope property, and thereby ensure that samples are
drawn from the desired target density, except in special
cases [35]. Thus, the construction of the Moore rejection
sampler through interval methods, that enclose the

http://www.almob.org/content/4/1/1

target shape over the entire real continuum in any box
of the domain with machine-representable bounds, in a
manner that rigorously accounts for all sources of
numerical errors (see [36] for a discussion on error
control), naturally guarantees that the Moore rejection
samples are independent draws from the desired target.
Moreover, the target is allowed to be multivariate and/or
non-log-concave with possibly 'pathological' behavior,
as long as it has a well-defined interval extension. The
efficiency of MRS is not immune to the curse of
dimensionality and target DAG complexity. When the
DAG expression for the likelihood gets large, its natural
interval extension can have terrible over-enclosures of
the true range, which in turn forces the adaptive
refinement of the domain to be extremely fine for
efficient envelope construction. Thus, a naive application
of interval methods to targets with large DAGs can be
terribly inefficient. In such cases, sampler efficiency
rather than rigor is the issue. Thus, one may fail to
obtain samples in a reasonable time, rather than (as may
happen with non-rigorous methods) produce samples
from some unknown and undesired target.
There are several ways in which efficiency can be improved
for such cases. First, the particular structure of the target
DAG should be exploited to avoid any redundant computations. For example, sufficient statistics must be used to
dissolve symmetries in the DAG. Second, we can further
improve efficiency by limiting ourselves to differentiable
targets in Cn. Tighter enclosures of the range of f(t(i)) with
f(t(i)) can come from the enclosures of Taylor expansions of
f around the midpoint mid (t(i)) through interval-extended
automatic differentiation (e.g. [36]) that can then yield
tighter estimates of the integral enclosures [37]. Third, we
can employ pre-processing to improve efficiency. For
example, we can pre-enclose the range of a possibly rescaled
f over a partition of the domain and then obtain the
enclosure of f over some arbitrary t through a combination
of hash access and hull operations on the pre-enclosures.
Such a pre-enclosing technique reduces not only the
overestimation of target shapes with large DAGs but also
the computational cost incurred while performing interval
operations with processors that are optimized for floatingpoint arithmetic. In the next version of the MRS library we
plan to extend interval arithmetic beyond IR n to a class of
multi-dimensional data-structures related to regular subpavings (e.g. [38]) to improve the efficiency of our sampler.
Fourth, various contractors can be used to improve the range
enclosure in polynomial time (e.g. [38]). The most
promising contractors employ interval constraint propagation. Finally, efficiency at the possible cost of rigor can also
be gained (up to 30%) by foregoing directed rounding
during envelope construction.

Page 16 of 19
(page number not for citation purposes)

Algorithms for Molecular Biology 2009, 4:1

Poor sampler efficiency makes it currently impractical to
sample from trees with five leaves and 15 topologies.
However, one could use such triplets and quartets drawn
from the posterior distribution to stochastically amalgamate
and produce estimates of larger trees via fast amalgamating
algorithms (e.g. [39,40]), which may then be used to
combat the slow mixing in MCMC methods [2] by
providing a good set of initial trees. A collection of large
trees obtained through such stochastic amalgamations
would account for the effect of finite sample sizes (sequence
length) as well as the sensitivity of the amalgamating
algorithm itself to variation in the input vector of small tree
estimates. It would be interesting to investigate if such
stochastic amalgamations can help improve mixing of
MCMC algorithms on large tree spaces, albeit autovalidating rejection sampling via the natural interval
extension of the likelihood function may not be practical
for trees with more than four leaves.

Conclusion
None of the currently available punctual samplers can
rigorously produce independent and identically distributed
samples from the posterior distribution over phylogenetic
tree spaces, even for 3 or 4 taxa. We describe a new approach
for rigorously drawing samples from a target posterior
distribution over small phylogenetic tree spaces using the
theory of interval analysis. Our Moore rejection sampler
(MRS), being an auto-validating von Neumann rejection
sampler (RS), can produce independent samples from any
target shape f whose DAG expression f has a well-defined
natural interval extension f over a compact domain T . MRS
is said to be auto-validating because it automatically obtains
a proposal g that is easy to simulate from, and an envelope
ˆ
g that is guaranteed to satisfy the envelope condition (1).
MRS can circumvent the problems associated with (i)
heuristic convergence diagnostics in MCMC samplers and
(ii) pseudo-envelopes constructed via non-rigorous punctual methods in rejection samplers. When the target DAG is
large, MRS becomes inefficient and may fail to produce the
desired samples in a reasonable time, rather than (as may
happen with non-rigorous methods) produce samples from
some unknown and undesired target. MRS solves the open
problem of rigorously drawing independent and identically
distributed samples from the posterior distribution over
small rooted and unrooted phylogenetic tree spaces (3 or 4
taxa) based on any multiply-aligned sequence data.

Competing interests

http://www.almob.org/content/4/1/1

interface and refined the final implementation of the
algorithm. Both authors edited the manuscript.

Appendix
Likelihoods for the CFN model on unrooted triplets
Recall that the probability that Y mutates to R, or vice
versa, in time t is a(t):= (1 - e-2t)/2 and the stationary
distribution π(R) = π(Y) = 1/2. Next we apply Algorithm
2 to compute the likelihood l d i,q at a given site q which
could be one of l0(4t), l1(4t),..., l7(4t).
l 0( 4 t ) = p (R )PR ,R ( 4 t 1 )PR ,R ( 4 t 2 )PR ,R ( 4 t 3 ) + p ( Y )PY ,R ( 4 t 1 )PY,,R ( 4 t 2 )PY ,R ( 4 t 3 )
1
(1 − a( 4 t 1 ))(1 − a( 4 t 2 ))(1 − a( 4 t 3 )) + a( 4 t 1 )a( 4 t 2 )a( 4 t 3 )
=
2
4
4
4
4
4
4
1
(1 + e −2( t 1 ) )(1 + e −2( t 2 ) )(1 + e −2( t 3 ) ) + (1 − e −2( t 1 ) )(1 − e −2( t 2 ) )(1 − e −2( t 3 ) )
=
16
4
4
4
4
4
4
1
1 + e −2( t 1 + t 2 ) + e −2( t 2 + t 3 ) + e −2( t 1 + t 3 )
=
8

(

(

)

(

)

(19)
l1( 4 t ) = p (R )PR , Y ( 4 t 1 )PR , Y ( 4 t 2 )PR , Y ( 4 t 3 ) + p ( Y )PY , Y ( 4 t 1 )PY,, Y ( 4 t 2 )PY , Y ( 4 t 3 )
1
=
a( 4 t 1 )a( 4 t 2 )a( 4 t 3 ) + (1 − a( 4 t 1 ))(1 − a( 4 t 2 ))(1 − a( 4 t 3 ))
2
= l 0( 4 t )

(

)

(20)
l 2( 4 t ) = p (R )PR ,R ( 4 t 1 )PR ,R ( 4 t 2 )PR , Y ( 4 t 3 ) + p ( Y )PY ,R ( 4 t 1 )PY,,R ( 4 t 2 )PY , Y ( 4 t 3 )
1
=
(1 − a( 4 t 1 ))(1 − a( 4 t 2 ))a( 4 t 3 ) + a( 4 t 1 )a( 4 t 2 )(1 − a( 4 t 3 ))
2
4
4
4
4
4
4
1
=
(1 + e −2( t 1 ) )(1 + e −2( t 2 ) )(1 − e −2( t 3 ) ) + (1 − e −2( t 1 ) )(1 − e −2( t 2 ) )(1 + e −2( t 3 ) )
16
4
4
4
4
4
4
1
=
1 + e −2( t 1 + t 2 ) − e −2( t 2 + t 3 ) − e −2( t 1 + t 3 )
8

(

(

)

(

)

l 3( t ) = p (R )PR , Y ( t 1 )PR , Y ( t 2 )PR ,R ( t 3 ) + p ( Y )PY , Y ( t 1 )PY,, Y ( t 2 )PY ,R ( 4 t 3 )
1
=
a( 4 t 1 )a( 4 t 2 )(1 − a( 4 t 3 )) + (1 − a( 4 t 1 ))(1 − a( 4 t 2 ))a( 4 t 3 )
2
= l 2( 4 t )
4

4

4

4

4

(

)

(22)
l 4 ( 4 t ) = p (R )PR ,R ( 4 t 1 )PR , Y ( 4 t 2 )PR , Y ( 4 t 3 ) + p ( Y )PY ,R ( 4 t 1 )PY,, Y ( 4 t 2 )PY , Y ( 4 t 3 )
1
(1 − a( 4 t 1 ))a( 4 t 2 )a( 4 t 3 ) + a( 4 t 1 )(1 − a( 4 t 2 ))(1 − a( 4 t 3 ))
=
2
4
4
4
4
4
4
1
=
(1 + e −2( t 1 ) )(1 − e −2( t 2 ) )(1 − e −2( t 3 ) ) + (1 − e −2( t 1 ) )(1 + e −2( t 2 ) )(1 + e −2( t 3 ) )
16
4
4
4
4
4
4
1
=
1 − e −2( t 1 + t 2 ) + e −2( t 2 + t 3 ) − e −2( t 1 + t 3 )
8

(

(

)

(

)

)

(23)
l 5( 4 t ) = p (R )PR , Y ( 4 t 1 )PR ,R ( 4 t 2 )PR ,R ( 4 t 3 ) + p ( Y )PY , Y ( 4 t 1 )PY,,R ( 4 t 2 )PY ,R ( 4 t 3 )
1
=
a( 4 t 1 )(1 − a( 4 t 2 ))(1 − a( 4 t 3 )) + (1 − a( 4 t 1 ))a( 4 t 2 )a( 4 t 3 )
2
= l 4( 4 t )

(

)

(24)
l6( 4 t ) = p (R )PR ,R ( 4 t 1 )PR , Y ( 4 t 2 )PR ,R ( 4 t 3 ) + p ( Y )PY ,R ( 4 t 1 )PY,, Y ( 4 t 2 )PY ,R ( 4 t 3 )
1
=
(1 − a( 4 t 1 ))a( 4 t 2 )(1 − a( 4 t 3 )) + a( 4 t 1 )(1 − a( 4 t 2 ))a( 4 t 3 )
2
4
4
4
4
4
4
1
(1 + e −2( t 1 ) )(1 − e −2( t 2 ) )(1 + e −2( t 3 ) ) + (1 − e −2( t 1 ) )(1 + e −2( t 2 ) )(1 − e −2( t 3 ) )
=
16
4
4
4
4
4
4
1
=
1 − e −2( t 1 + t 2 ) − e −2( t 2 + t 3 ) + e −2( t 1 + t 3 )
8

(

(

)

(

)

)

(25)
l7( t ) = p (R )PR , Y ( t 1 )PR ,R ( t 2 )PR , Y ( t 3 ) + p ( Y )PY , Y ( t 1 )PY,,R ( t 2 )PY , Y ( 4 t 3 )
1
=
a( 4 t 1 )(1 − a( 4 t 2 ))a( 4 t 3 ) + (1 − a( 4 t 1 ))a( 4 t 2 )(1 − a( 4 t 3 ))
2
= l 6( 4 t )
4

RS developed the basic algorithm, analyzed the data and
wrote the first draft. TY improved the object-oriented

)

(21)
4

The authors declare that they have no competing interests.

Authors' contributions

)

4

(

4

4

4

4

)

(26)

Page 17 of 19
(page number not for citation purposes)

Algorithms for Molecular Biology 2009, 4:1

Proof of Theorem 1 (cf. [37])
Since any real arithmetic operation x ⋆ y, where ⋆ Œ {+, - ×,/}
and x, y Œ R, is a continuous function x ⋆y := ⋆(x, y): R ⊗ R ↦
R, except when y = 0 under / operation. Since x and y are
simply connected compact intervals, so is their Cartesian
product x ⊗ y. On such a domain x ⊗ y, the continuity of ⋆(x,
y) (except when ⋆ =/and 0 Œ y) ensures the attainment of a
minimum, a maximum and all intermediate values. Therefore, with the exception of the case when ⋆ = / and 0 Œ y, the
range x ⋆ y has an interval form [min (x ⋆y), max (x ⋆y)],
where the min and max are taken over all pairs (x, y) Œ x ⊗ y.
Fortunately, we do not have to evaluate x ⋆y over every (x, y)
Œ x ⊗ y to find the global min and global max of ⋆(x, y) over
x ⊗ y, because the monotonicity of the ⋆(x, y*) in terms of x
Œ x for any fixed y* Œ y implies that the extremal values are
attained on the boundary of x ⊗ y, i.e. the set {x, y, x , and
y }. Thus the theorem can be verified by examining the
finitely many boundary cases.
Proof of Theorem 2
x ⋆ y = {x ⋆y : x Œ x, y Œ y} ⊆ {x ⋆y : x Œ x', y Œ y'} = x' ⋆ y'.

Proof of Theorem 3 (cf. [37])
Since f(y) is well-defined, we will not run into division
by zero, and therefore (i) follows from the repeated
invocation of Theorem 2. We can prove (ii) by contradiction. Suppose range(f; x) ⊈ f(x). Then there exists x Œ
x, such that f(x) Œ range(f; x) but f(x) ∉ f(x). This in turn
implies that f(x) = f([x, x]) ∉ f(x), which contradicts (i).
Therefore, our supposition cannot be true and we have
proved (ii) range(f; x) ⊆ f(x).
Proof of Theorem 4 (cf. [37])
Any elementary function f Œ E with expression f is defined by
the recursion 9 on its sub-expressions fi where i Œ {1,..., n}
according to its DAG. If f(x) = p(x)/q(x) is a rational function,
then the theorem already holds by Theorem 3, and if f Œ S
then the theorem holds because the range enclosure is exact
for standard functions. Thus it suffices to show that if the
theorem holds for f1, f2 Œ E , then the theorem also holds for
f1 ⋆ f2, where ⋆ Œ {+, -,/, ×, ◦}. By ◦ we mean the composition
operator. Since the proof is analogous for all five operators,
we only focus on the ◦ operator. Since f is well-defined on its
domain y, neither the real-valued f nor any of its subexpressions fi has singularities in its respective domain yi
induced by y. In particular f2 is continuous on any x2 and x′2
such that x2 ⊆ x′2 ⊆ y2 implying the compactness of f2(x2) =:
w2 and f2( x′2 ) =: w′2 , respectively. By our assumption that f1
and f2 are inclusion isotonic we have that w2 ⊆ w′2 and also
that
f 1 f 2( x 2 ) = f 1(f 2( x 2 )) = f 1(w 2 ) ⊆ f 1(w′2 ) = f 1(f 2( x′2 )) = f 1 f 2( x′2 )

http://www.almob.org/content/4/1/1

The range enclosure is a consequence of inclusion
isotony by an argument identical to that given in the
proof for Theorem 3.
Proof of Theorem 5 (cf. [37])
The proof is given by an induction on the DAG for f
similar to the proof of Theorem 4 (See [37]).
Proof of Theorem 6
Let the domain T of the target f• be an element of IR n .
ˆ
From (15) and (14) observe that g T (t ) = g T (t )N g T . Let
ˆ
us define the following two subsets of Rn+1,
B( g T ) = {(v , u) : v ∈ T, 0 ≤ u ≤ g T (v)}, and B( f ) = {(v , u) : v ∈ T, 0 ≤ u ≤ f (v)}.

Algorithm 1 first produces a sample from the random
vector (V, U) that is uniformly distributed in B( g T ) . We
can see this by letting h(v, u) denote the joint density of
(V, U) and h(u|v) denote the conditional density of U
given V = v. Then,

⎧ g T (v)h(u | v) if (v , u) ∈ B( g T )
⎪
h(v , u) = ⎨
otherwise.
⎪
⎩0
Since we sample a height u for a given v from the
Uniform [0, g T ] distribution,
ˆ

⎧ ( g T (v)) −1 = ( g T (v))N T ) −1 if u ∈ [0, g T (v)]
⎪
g
h(u | v) = ⎨
otherwise.
⎪0
⎩
Therefore,
⎧ g T (v)h(u | v) = g T (v)( g T (v)N T ) −1 = (N T ) −1 if (v , u) ∈ B( g T )
⎪
g
g
h(v , u) = ⎨
otherwise.
⎪0
⎩

Thus, we have shown that the joint
random vector (V, U) initially produced
is uniformly distributed on B( g T ) . The
ship also makes geometric sense since
B( g T ) is exactly N g T .
ˆ

density of the
by Algorithm 1
above relationthe volume of

Now, let (T, S) be the accepted random vector during the
accept/reject step of Algorithm 1, i.e.

(T , S) = (V , U ) ⇔ (V , U ) ∈ B( f ) ⊆ B( g T ).
Then, the uniform distribution of (V, U) on B( g T )
implies the uniform distribution of (T, S) on B (f). Since
the volume of B (f) is Nf, the density of (T, S) is
identically 1/Nf on B (f) and 0 elsewhere. Hence, the
marginal density of T on T is

Page 18 of 19
(page number not for citation purposes)

Algorithms for Molecular Biology 2009, 4:1

∫

f (t )

0

1 / N f dh = 1 / N f
= 1/ N f
=

f i (t ).

∫
∫

http://www.almob.org/content/4/1/1

12.

f (t )

1 dh

13.

0
N f f i (t )

0

1 dh, ∵ f i (t ) = f (t ) / N f

14.
15.

Thus, we have shown that the accepted random vector T
has the desired density f•.

16.
17.
18.

Proof of Theorem 7
Due to Theorem 5,

19.

(i
(i
(i
wid (t W) ) = O(1 / W ) ⇒ dist ∞ (range (f ; t W) , f (t W) )) = O(1 / W )

20.

(i
⇒ wid (f (t W) )) = O(1 / W ), ∵ f ∈ E L .

21.

Therefore
|U W |

∑ ( wid

W

(i
(i
(t W) ) ⋅ f (t W) )

i =1

) = w∑ f ([ t + (i − 1)w, t + iw]),
i =1

∑

W
i =1

23.
24.

and we have
wid (w

22.

i
f (t W )) = O(1 / W ) ⇒

25.

A( U W ) = 1 − O(1 / W ).

Therefore the lower bound for the acceptance probability
A( U W ) of MRS approaches 1 no slower than linearly
with the refinement of T by U W . Note that this should
hold for a general nonuniform partition with w replaced
by the mesh.

26.
27.
28.
29.

Acknowledgements
R.S. is a Research Fellow of the Royal Commission for the Exhibition of
1851. This was partly supported by a joint NSF/NIGMS grant DMS-0201037. Many thanks to Rob Strawderman, Warwick Tucker and Stephane
Aris-Brosou for constructive comments, Joe Felsenstein for clarifying the
transition probabilities under the HKY model and Ziheng Yang for various
clarifications and encouragement.

References
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.

Jones G and Hobert J: Honest exploration of intractable
probability distributions via Markov chain Monte Carlo.
Statistical Science 2001, 16(4):312–334.
Mossel E and Vigoda E: Phylogenetic MCMC algorithms are
misleading on mixtures of trees. Science 2005, 309:2207–2209.
von Neumann J: Various techniques used in connection with
random digits. John Von Neumann, Collected Works Oxford
University Press; 1963, V:.
Walker A: An efficient method for generating discrete
random variables with general distributions. ACM Trans on
Mathematical Software 1977, 3:253–256.
Sainudiin R: Machine interval experiments. pHd dissertation
Cornell University, Ithaca, New York; 2005.
Moore R: Interval analysis Prentice-Hall; 1967.
Semple C and Steel M: Phylogenetics Oxford University Press; 2003.
Felsenstein J: Inferring phylogenies Sunderland, MA: Sinauer Associates; 2003.
Yang Z: Computational molecular evolution UK: Oxford University
Press; 2006.
Moore R: Methods and applications of interval analysis Philadelphia,
Pennsylvania: SIAM; 1979.
Alefeld G and Herzberger J: An introduction to interval computations
Academic press; 1983.

30.

31.
32.
33.

34.
35.
36.
37.
38.
39.
40.

Hammer R, Hocks M, Kulisch U and Ratz D: C++ toolbox for verified
computing: basic numerical problems Springer-Verlag; 1995.
Kulisch U, Lohner R, Facius A and Eds: Perspectives on encolsure
methods Springer-Verlag; 2001.
Matsumoto M and Nishimura T: Mersenne Twister: A 623dimensionally equidistributed uniform pseudo-random
number generator. ACM Trans Model Comput Simul 1998, 8:3–30.
Williams D: Weighing the Odds: A Course in Probability and Statistics
Cambridge University Press; 2001.
Felsenstein J: Evolutionary trees from DNA sequences: a
maximum likelihood approach. Jnl Mol Evol 1981, 17:368–376.
Yang Z: Complexity of the simplest phylogenetic estimation
problem. Proceedings Royal Soc London B Biol Sci 2000, 267:109–119.
Evans W, Kenyon C, Peres Y and Schulman L: Broadcasting on
trees and the Ising model. Advances in Applied Probability 2000,
10:410–433.
Neyman J: Molecular studies of evolution: a source of novel
statistical problems. Statistical decision theory and related topics
New York Academy Press: Gupta S, Yackel J 1971, 1–27.
Jukes T and Cantor C: Evolution of protein molecules.
Mammalian Protein Metabolism New York Academic Press: Munro
H 1969, 21–32.
Saitou N: Property and efficiency of the maximum likelihood
method for molecular phylogeny. Jnl Mol Evol 1988,
27:261–273.
Yang Z: Statistical properties of the maximum likelihood
method of phylogenetic estimation and comparison with
distance matrix methods. Syst Biol 1994, 43:329–342.
Hosten S, Khetan A and Sturmfels B: Solving the likelihood
equations. Found Comput Math 2005, 5(4):389–407.
Casanellas M, Garcia L and Sullivant S: Catalog of small trees.
Algebraic statistics for computational biology Cambridge University
Press: Pachter L, Sturmfels B 2005, 291–304.
Hasegawa M, Kishino H and Yano T: Dating of the human-ape
splitting by a molecular clock of mitochondrial DNA. Jnl Mol
Evol 1985, 22:160–174.
Sainudiin R and Yoshida R: Applications of interval methods to
phylogenetic trees. Algebraic statistics for computational biology
Cambridge University Press: Pachter L, Sturmfels B 2005, 359–374.
Brown W, Prager E, Wang A and Wilson A: Mitochondrial DNA
sequences of primates, tempo and mode of evolution. Jnl Mol
Evol 1982, 18:225–239.
Marsaglia G: Generating discrete random numbers in a
computer. Comm ACM 1963, 6:37–38.
Galassi M, Davies J, Theiler J, Gough B, Jungman G, Booth M and
Rossi F: GNU Scientific Library Reference Manual Network Theory Ltd;
22003 http://www.gnu.org/software/gsl/.
Hofschuster and Krämer : C-XSC 2.0: A C++ library for
extended scientific computing. Numerical software with result
verification, of Lecture notes in computer science Springer-Verlag: Alt R,
Frommer A, Kearfott R, Luther W 2004, 2991:15–35.
Rannala B and Yang Z: Probability distribution of molecular
evolutionary trees: a new method of phylogenetic inference.
Jnl Mol Evol 1996, 43:304–311.
Yang Z and Rannala B: Branch-length prior in uences Bayesian
posterior probability of phylogeny. Syst Biol 2005, 54:455–470.
Green R, Krause J, Ptak S, Briggs A, Ronan M, Simons J, Du L,
Egholm M, Rothberg J, Paunovic M and Pääbo S: Analysis of one
million base pairs of Neandertal DNA. Nature 2006,
444:330–336.
Efron B, Halloran E and Holmes S: Bootstrap confidence levels
for phylogenetic trees. Proc Natl Acad Sci 1996, 93:13429–13429.
Gilks W and Wild P: Adaptive rejection sampling for Gibbs
sampling. Applied Statistics 1992, 41:337–348.
Kulisch U: Advanced arithmetic for the digital computer,
interval arithmetic revisited. Perspectives on encolsure methods
Springer-Verlag: Kulisch U, Lohner R, Facius A 2001, 50–70.
Tucker W: Auto-validating numerical methods.Lecture notes,
Uppsala University; 2004.
Jaulin L, Kieffer M, Didrit O and Walter E: Applied interval analysis:
with examples in parameter and state estimation, robust control and
robotics Springer-Verlag; 2004.
Strimmer K and von Haeseler A: Quartet puzzling: A quartet
maximum likelihood method for reconstructing tree topologies. Mol Biol Evol 1996, 13:964–969.
Levy D, Yoshida R and Pachter L: Beyond pairwise distances:
neighbor joining with phylogenetic diversity estimates. Mol
Biol Evol 2006, 23:491–498.

Page 19 of 19
(page number not for citation purposes)

</pre>
</body>
</html>
